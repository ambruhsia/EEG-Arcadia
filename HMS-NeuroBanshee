{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"},{"sourceId":7392775,"sourceType":"datasetVersion","datasetId":4297782}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/amritarajput54/hms-neurobanshee?scriptVersionId=227436996\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, udf, explode, array\nfrom pyspark.sql.types import DoubleType, ArrayType, StringType\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.stat import Correlation\n\n# Initialize Spark Session\nspark = SparkSession.builder \\\n    .appName(\"EEG Analysis\") \\\n    .getOrCreate()\n\n# Path to data\nBASE_PATH = '/kaggle/input/hms-harmful-brain-activity-classification/'\nFILE_PATH = BASE_PATH + 'train_eegs/1000913311.parquet'\n\n\n\n# # Shutdown Spark\n# spark.stop()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:05:43.060661Z","iopub.execute_input":"2025-03-13T17:05:43.060959Z","iopub.status.idle":"2025-03-13T17:05:51.305022Z","shell.execute_reply.started":"2025-03-13T17:05:43.060935Z","shell.execute_reply":"2025-03-13T17:05:51.303949Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df_eeg = spark.read.parquet(FILE_PATH)\ndf_eeg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:05:52.545222Z","iopub.execute_input":"2025-03-13T17:05:52.545892Z","iopub.status.idle":"2025-03-13T17:05:57.151098Z","shell.execute_reply.started":"2025-03-13T17:05:52.545856Z","shell.execute_reply":"2025-03-13T17:05:57.150058Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DataFrame[Fp1: float, F3: float, C3: float, P3: float, F7: float, T3: float, T5: float, O1: float, Fz: float, Cz: float, Pz: float, Fp2: float, F4: float, C4: float, P4: float, F8: float, T4: float, T6: float, O2: float, EKG: float]"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":" df_eeg.show(5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:05:58.126207Z","iopub.execute_input":"2025-03-13T17:05:58.126593Z","iopub.status.idle":"2025-03-13T17:06:01.74591Z","shell.execute_reply.started":"2025-03-13T17:05:58.126563Z","shell.execute_reply":"2025-03-13T17:06:01.744485Z"}},"outputs":[{"name":"stdout","text":"+-------+-------+------+------+-------+-------+------+------+------+------+------+------+------+-------+------+------+------+------+-------+--------+\n|    Fp1|     F3|    C3|    P3|     F7|     T3|    T5|    O1|    Fz|    Cz|    Pz|   Fp2|    F4|     C4|    P4|    F8|    T4|    T6|     O2|     EKG|\n+-------+-------+------+------+-------+-------+------+------+------+------+------+------+------+-------+------+------+------+------+-------+--------+\n|-105.85| -89.23|-79.46|-49.23| -99.73| -87.77|-53.33|-50.74|-32.25| -42.1|-43.27|-88.73|-74.41| -92.46|-58.93|-75.74|-59.47|  8.21|  66.49| 1404.93|\n| -85.47| -75.07|-60.26|-38.92| -73.08| -87.51|-39.68|-35.63|-76.84|-62.74|-43.04|-68.63|-61.69| -69.32|-35.79| -58.9|-41.66|196.19| 230.67| 3402.67|\n|   8.84|  34.85| 56.43| 67.97|   48.1|  25.35| 80.25| 48.06|  6.72| 37.88|  61.0| 16.58| 55.06|  45.02| 70.53| 47.82| 72.03|-67.18|-171.31| -3565.8|\n| -56.32| -37.28| -28.1| -2.82| -43.43| -35.05|  3.91|-12.66|  8.65|  3.83|  4.18| -51.9|-21.89| -41.33|-11.58|-27.04|-11.73| -91.0| -81.19|-1280.93|\n|-110.14|-104.52|-96.88|-70.25|-111.66|-114.43|-71.83|-61.92|-76.15|-79.78|-67.48|-99.03|-93.61|-104.41|-70.07|-89.25|-77.26|155.73| 264.85| 4325.37|\n+-------+-------+------+------+-------+-------+------+------+------+------+------+------+------+-------+------+------+------+------+-------+--------+\nonly showing top 5 rows\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":" df = spark.read.csv(\"/kaggle/input/hms-harmful-brain-activity-classification/train.csv\", \n                     header=True, inferSchema=True)\n\n# Display the first few rows\ndf.show(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:06:06.129127Z","iopub.execute_input":"2025-03-13T17:06:06.129515Z","iopub.status.idle":"2025-03-13T17:06:08.396985Z","shell.execute_reply.started":"2025-03-13T17:06:06.129468Z","shell.execute_reply":"2025-03-13T17:06:08.395236Z"}},"outputs":[{"name":"stdout","text":"+----------+----------+------------------------+--------------+------------------+--------------------------------+----------+----------+----------------+------------+--------+--------+---------+---------+----------+\n|    eeg_id|eeg_sub_id|eeg_label_offset_seconds|spectrogram_id|spectrogram_sub_id|spectrogram_label_offset_seconds|  label_id|patient_id|expert_consensus|seizure_vote|lpd_vote|gpd_vote|lrda_vote|grda_vote|other_vote|\n+----------+----------+------------------------+--------------+------------------+--------------------------------+----------+----------+----------------+------------+--------+--------+---------+---------+----------+\n|1628180742|         0|                     0.0|        353733|                 0|                             0.0| 127492639|     42516|         Seizure|           3|       0|       0|        0|        0|         0|\n|1628180742|         1|                     6.0|        353733|                 1|                             6.0|3887563113|     42516|         Seizure|           3|       0|       0|        0|        0|         0|\n|1628180742|         2|                     8.0|        353733|                 2|                             8.0|1142670488|     42516|         Seizure|           3|       0|       0|        0|        0|         0|\n|1628180742|         3|                    18.0|        353733|                 3|                            18.0|2718991173|     42516|         Seizure|           3|       0|       0|        0|        0|         0|\n|1628180742|         4|                    24.0|        353733|                 4|                            24.0|3080632009|     42516|         Seizure|           3|       0|       0|        0|        0|         0|\n+----------+----------+------------------------+--------------+------------------+--------------------------------+----------+----------+----------------+------------+--------+--------+---------+---------+----------+\nonly showing top 5 rows\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# if \"eval\" in FLAGS:\n#     import os\n\n#     # Set the environment variable\n#     os.environ[\"PYSPARK_PIN_THREAD\"] = \"False\"\n#     # spark.builder.config(\"spark.jars.packages\", \"org.mlflow.mlflow-spark\")\n#     import mlflow\n\n#     # mlflow.set_tracking_uri(\"http://127.0.0.0:5000\")\n#     mlflow.set_tracking_uri(\"http://localhost:5000\")\n#     mlflow.autolog()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # Extract column names\ncolumns = df.columns\nTARGETS = columns[-6:]\n\n# Print shape (row count and column count)\nprint(\"Train shape:\", (df.count(), len(columns)))\n\n# Display target column names\nprint(\"Target Labels:\", TARGETS)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:06:11.518287Z","iopub.execute_input":"2025-03-13T17:06:11.518669Z","iopub.status.idle":"2025-03-13T17:06:12.379651Z","shell.execute_reply.started":"2025-03-13T17:06:11.518638Z","shell.execute_reply":"2025-03-13T17:06:12.378569Z"}},"outputs":[{"name":"stdout","text":"Train shape: (106800, 15)\nTarget Labels: ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from pyspark.sql.functions import col\n\n# Count the number of occurrences of each EEG pattern\nfor target in TARGETS:\n    df.groupBy(target).count().orderBy(col(\"count\").desc()).show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:06:14.255816Z","iopub.execute_input":"2025-03-13T17:06:14.256116Z","iopub.status.idle":"2025-03-13T17:06:17.975967Z","shell.execute_reply.started":"2025-03-13T17:06:14.256092Z","shell.execute_reply":"2025-03-13T17:06:17.974959Z"}},"outputs":[{"name":"stdout","text":"+------------+-----+\n|seizure_vote|count|\n+------------+-----+\n|           0|73906|\n|           3|19520|\n|           1| 6475|\n|           2| 2329|\n|           5| 1825|\n|           4| 1745|\n|           6|  336|\n|           7|  313|\n|           8|   91|\n|           9|   57|\n|          10|   54|\n|          15|   36|\n|          13|   30|\n|          11|   29|\n|          14|   25|\n|          12|   22|\n|          19|    4|\n|          16|    3|\n+------------+-----+\n\n+--------+-----+\n|lpd_vote|count|\n+--------+-----+\n|       0|77675|\n|       1| 9680|\n|       2| 4618|\n|       3| 4011|\n|       4| 2290|\n|       5| 1323|\n|       6| 1065|\n|       7|  863|\n|      13|  769|\n|      14|  739|\n|      10|  629|\n|       8|  616|\n|      12|  589|\n|       9|  574|\n|      15|  557|\n|      11|  545|\n|      17|  120|\n|      16|   92|\n|      18|   45|\n+--------+-----+\n\n+--------+-----+\n|gpd_vote|count|\n+--------+-----+\n|       0|82027|\n|       1| 5643|\n|       2| 4352|\n|       3| 2756|\n|      10| 2052|\n|      11| 1445|\n|       9| 1200|\n|       4| 1163|\n|      12| 1054|\n|      13|  941|\n|       5|  909|\n|       8|  861|\n|       7|  774|\n|       6|  573|\n|      15|  510|\n|      14|  424|\n|      16|  116|\n+--------+-----+\n\n+---------+-----+\n|lrda_vote|count|\n+---------+-----+\n|        0|77294|\n|        1| 8177|\n|        3| 6597|\n|        2| 6122|\n|        5| 1621|\n|        6| 1449|\n|        4| 1279|\n|        7| 1190|\n|        9|  802|\n|        8|  799|\n|       11|  469|\n|       13|  357|\n|       10|  292|\n|       12|  246|\n|       15|   55|\n|       14|   51|\n+---------+-----+\n\n+---------+-----+\n|grda_vote|count|\n+---------+-----+\n|        0|73101|\n|        3|12643|\n|        1| 9508|\n|        2| 4219|\n|        6| 1058|\n|        5|  983|\n|        4|  927|\n|        8|  923|\n|        7|  900|\n|       13|  866|\n|        9|  573|\n|       10|  471|\n|       12|  272|\n|       11|  269|\n|       14|   62|\n|       15|   25|\n+---------+-----+\n\n+----------+-----+\n|other_vote|count|\n+----------+-----+\n|         0|58167|\n|         1|13471|\n|         2| 9782|\n|         3| 7286|\n|         4| 3081|\n|         5| 3016|\n|         6| 2153|\n|         7| 1544|\n|         9| 1228|\n|         8| 1061|\n|        10|  914|\n|        11|  777|\n|        12|  761|\n|        13|  615|\n|        14|  563|\n|        17|  545|\n|        18|  454|\n|        15|  425|\n|        16|  326|\n|        20|  303|\n+----------+-----+\nonly showing top 20 rows\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from pyspark.sql.functions import first, min, max, sum, col\n\n# Select the first spectrogram_id and earliest spectrogram_label_offset_seconds for each eeg_id\ntrain = df.groupBy(\"eeg_id\").agg(\n    first(\"spectrogram_id\").alias(\"spec_id\"),\n    min(\"spectrogram_label_offset_seconds\").alias(\"min\")\n)\n\n# Find the latest spectrogram_label_offset_seconds\ntmp = df.groupBy(\"eeg_id\").agg(max(\"spectrogram_label_offset_seconds\").alias(\"max\"))\ntrain = train.join(tmp, on=\"eeg_id\", how=\"left\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:06:22.370508Z","iopub.execute_input":"2025-03-13T17:06:22.370819Z","iopub.status.idle":"2025-03-13T17:06:22.473981Z","shell.execute_reply.started":"2025-03-13T17:06:22.370795Z","shell.execute_reply":"2025-03-13T17:06:22.472766Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"tmp = df.groupBy(\"eeg_id\").agg(first(\"patient_id\").alias(\"patient_id\"))\ntrain = train.join(tmp, on=\"eeg_id\", how=\"left\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:06:25.351728Z","iopub.execute_input":"2025-03-13T17:06:25.352031Z","iopub.status.idle":"2025-03-13T17:06:25.393151Z","shell.execute_reply.started":"2025-03-13T17:06:25.352006Z","shell.execute_reply":"2025-03-13T17:06:25.391961Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"target_agg = df.groupBy(\"eeg_id\").agg(\n    *[sum(col(t)).alias(t) for t in TARGETS]  # Sum votes for each target label\n)\n\ntrain = train.join(target_agg, on=\"eeg_id\", how=\"left\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:06:27.401021Z","iopub.execute_input":"2025-03-13T17:06:27.401335Z","iopub.status.idle":"2025-03-13T17:06:27.470414Z","shell.execute_reply.started":"2025-03-13T17:06:27.401308Z","shell.execute_reply":"2025-03-13T17:06:27.469436Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from functools import reduce\nfrom pyspark.sql.functions import col\n\n# Compute total votes as the sum of all target columns\ntrain = train.withColumn(\n    \"total_votes\",\n    reduce(lambda x, y: x + y, [col(t) for t in TARGETS])  # Sum up all target columns\n)\nfrom pyspark.sql.functions import when\n\n# Avoid division by zero using `when`\nfor t in TARGETS:\n    train = train.withColumn(t, when(col(\"total_votes\") > 0, col(t) / col(\"total_votes\")).otherwise(0))\n\n# Drop `total_votes` after normalization\ntrain = train.drop(\"total_votes\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:06:29.528362Z","iopub.execute_input":"2025-03-13T17:06:29.528741Z","iopub.status.idle":"2025-03-13T17:06:29.728127Z","shell.execute_reply.started":"2025-03-13T17:06:29.52871Z","shell.execute_reply":"2025-03-13T17:06:29.727064Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"tmp = df.groupBy(\"eeg_id\").agg(first(\"expert_consensus\").alias(\"target\"))\ntrain = train.join(tmp, on=\"eeg_id\", how=\"left\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:06:31.663261Z","iopub.execute_input":"2025-03-13T17:06:31.663648Z","iopub.status.idle":"2025-03-13T17:06:31.70169Z","shell.execute_reply.started":"2025-03-13T17:06:31.663615Z","shell.execute_reply":"2025-03-13T17:06:31.70069Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"train = train.orderBy(\"eeg_id\")  # Optional: Order by EEG ID\ntrain.show(5)\n\nprint(\"Train non-overlapping EEG ID shape:\", (train.count(), len(train.columns)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:06:33.921944Z","iopub.execute_input":"2025-03-13T17:06:33.92225Z","iopub.status.idle":"2025-03-13T17:06:38.338393Z","shell.execute_reply.started":"2025-03-13T17:06:33.922225Z","shell.execute_reply":"2025-03-13T17:06:38.337248Z"}},"outputs":[{"name":"stdout","text":"+------+----------+------+------+----------+------------+------------------+--------+-------------------+-------------------+-------------------+------+\n|eeg_id|   spec_id|   min|   max|patient_id|seizure_vote|          lpd_vote|gpd_vote|          lrda_vote|          grda_vote|         other_vote|target|\n+------+----------+------+------+----------+------------+------------------+--------+-------------------+-------------------+-------------------+------+\n|568657| 789577333|   0.0|  16.0|     20654|         0.0|               0.0|    0.25|                0.0|0.16666666666666666| 0.5833333333333334| Other|\n|582999|1552638400|   0.0|  38.0|     20230|         0.0|0.8571428571428571|     0.0|0.07142857142857142|                0.0|0.07142857142857142|   LPD|\n|642382|  14960202|1008.0|1032.0|      5955|         0.0|               0.0|     0.0|                0.0|                0.0|                1.0| Other|\n|751790| 618728447| 908.0| 908.0|     38549|         0.0|               0.0|     1.0|                0.0|                0.0|                0.0|   GPD|\n|778705|  52296320|   0.0|   0.0|     40955|         0.0|               0.0|     0.0|                0.0|                0.0|                1.0| Other|\n+------+----------+------+------+----------+------------+------------------+--------+-------------------+-------------------+-------------------+------+\nonly showing top 5 rows\n\nTrain non-overlapping EEG ID shape: (17089, 12)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, mean, min\nimport os\n\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"EEGFeatureEngineering\").getOrCreate()\n\n# Define path to spectrogram parquet files\nPATH = \"/kaggle/input/hms-harmful-brain-activity-classification/train_spectrograms/\"\n\n# List all parquet files\nfiles = [f for f in os.listdir(PATH) if f.endswith(\".parquet\")]\n\nprint(f\"There are {len(files)} spectrogram parquet files.\")\n\n# Read all parquet files into a single PySpark DataFrame\ndf = spark.read.parquet(PATH)\n\n# Select relevant columns (assuming frequency columns are named like 'freq_1', 'freq_2', etc.)\nfreq_cols = [c for c in df.columns if \"freq_\" in c]\n\n#\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:06:43.62982Z","iopub.execute_input":"2025-03-13T17:06:43.630151Z","iopub.status.idle":"2025-03-13T17:07:29.908308Z","shell.execute_reply.started":"2025-03-13T17:06:43.630122Z","shell.execute_reply":"2025-03-13T17:07:29.907321Z"}},"outputs":[{"name":"stdout","text":"There are 11138 spectrogram parquet files.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nimport numpy as np\n\n# # Initialize Spark Session\n# spark = SparkSession.builder.appName(\"SpectrogramProcessing\").getOrCreate()\n\nif READ_SPEC_FILES:    \n    spectrograms = {}\n\n    # Read all Parquet files into a single PySpark DataFrame\n    df = spark.read.parquet(f\"{PATH}*.parquet\")\n\n    # Convert filenames into `spec_id`\n    df = df.withColumn(\"spec_id\", col(\"filename\").substr(1, 10).cast(\"int\"))\n\n    # Collect as a dictionary {spec_id: spectrogram array}\n    spectrograms = {row[\"spec_id\"]: np.array(row[1:]) for row in df.collect()}\n\nelse:\n    spectrograms = np.load('/kaggle/input/brain-spectrograms/specs.npy', allow_pickle=True).item()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:16:28.606005Z","iopub.execute_input":"2025-03-13T17:16:28.606367Z","iopub.status.idle":"2025-03-13T17:17:12.133826Z","shell.execute_reply.started":"2025-03-13T17:16:28.606342Z","shell.execute_reply":"2025-03-13T17:17:12.132915Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"\n\nREAD_SPEC_FILES = False # If READ_SPEC_FILES is False, the code reads the combined file instead of individual files.\nFEATURE_ENGINEER = True\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:12:07.274774Z","iopub.execute_input":"2025-03-13T17:12:07.275129Z","iopub.status.idle":"2025-03-13T17:12:07.279466Z","shell.execute_reply.started":"2025-03-13T17:12:07.275104Z","shell.execute_reply":"2025-03-13T17:12:07.278326Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"\n\n# %time\n# # ENGINEER FEATURES\n# import warnings\n# warnings.filterwarnings('ignore')\n\n# # The code generates features from the spectrogram data for use in a model \n# # The features are derived by calculating the mean and minimum values over time for each of the 400 spectrogram frequencies.\n# # Two types of windows are used for these calculations:\n# # A 10-minute window (_mean_10m, _min_10m).\n# # A 20-second window (_mean_20s, _min_20s).\n# # This process results in 1600 features (400 features × 4 calculations) for each EEG ID.\n\n#  # = pd.read_parquet(f'{PATH}1000086677.parquet').columns[1:]\n# SPEC_COLS = spark.read.parquet(f'{PATH}1000086677.parquet').columns[1:]\n\n# # # Get all columns except the first one\n# # SPEC_COLS = df\n\n# FEATURES = [f'{c}_mean_10m' for c in SPEC_COLS]\n# FEATURES += [f'{c}_min_10m' for c in SPEC_COLS]\n# FEATURES += [f'{c}_mean_20s' for c in SPEC_COLS]\n# FEATURES += [f'{c}_min_20s' for c in SPEC_COLS]\n# print(f'We are creating { len(FEATURES)} features for { (train).count()} rows... ',end='')\n\n\n# # A data matrix data is initialized to store the new features for each eeg_id in the train DataFrame.\n# # For each row in train, the code calculates the mean and minimum values within the specified 10-minute and 20-second windows.\n# # These calculated values are then stored in the data matrix.\n# # Finally, the matrix is added to the train DataFrame as new columns.\n\n# if FEATURE_ENGINEER:\n#     data = np.zeros(( (train.count()),len(FEATURES)))\n#     for k in range( (train).count()):\n#         if k%100==0: print(k,', ',end='')\n#         row = train.collect()[k]  # ✅ Correct for retrieving a specific row\n\n#         r = int( (row['min'] + row['max'])//4 ) \n        \n#         # 10 MINUTE WINDOW FEATURES (MEANS and MINS)\n#         x = np.nanmean(spectrograms[row.spec_id][r:r+300,:],axis=0)\n#         data[k,:400] = x\n#         x = np.nanmin(spectrograms[row.spec_id][r:r+300,:],axis=0)\n#         data[k,400:800] = x\n        \n#         # 20 SECOND WINDOW FEATURES (MEANS and MINS)\n#         x = np.nanmean(spectrograms[row.spec_id][r+145:r+155,:],axis=0)\n#         data[k,800:1200] = x\n#         x = np.nanmin(spectrograms[row.spec_id][r+145:r+155,:],axis=0)\n#         data[k,1200:1600] = x\n\n#     train[FEATURES] = data\n# else:\n#     train = pd.read_parquet('/kaggle/input/brain-spectrograms/train.pqt')\n# print()\n# print('New train shape:',train.shape)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:17:20.603908Z","iopub.execute_input":"2025-03-13T17:17:20.604872Z","iopub.status.idle":"2025-03-13T18:03:02.113945Z","shell.execute_reply.started":"2025-03-13T17:17:20.60482Z","shell.execute_reply":"2025-03-13T18:03:02.112568Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 3 µs, sys: 0 ns, total: 3 µs\nWall time: 6.2 µs\nWe are creating 1600 features for 17089 rows... 0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , 3900 , 4000 , 4100 , 4200 , 4300 , 4400 , 4500 , 4600 , 4700 , 4800 , 4900 , 5000 , 5100 , 5200 , 5300 , 5400 , 5500 , 5600 , 5700 , 5800 , 5900 , 6000 , 6100 , 6200 , 6300 , 6400 , 6500 , 6600 , 6700 , 6800 , 6900 , 7000 , 7100 , 7200 , 7300 , 7400 , 7500 , 7600 , 7700 , 7800 , 7900 , 8000 , 8100 , 8200 , 8300 , 8400 , 8500 , 8600 , 8700 , 8800 , 8900 , 9000 , 9100 , 9200 , 9300 , 9400 , 9500 , 9600 , 9700 , 9800 , 9900 , 10000 , 10100 , 10200 , 10300 , 10400 , 10500 , 10600 , 10700 , 10800 , 10900 , 11000 , 11100 , 11200 , 11300 , 11400 , 11500 , 11600 , 11700 , 11800 , 11900 , 12000 , 12100 , 12200 , 12300 , 12400 , 12500 , 12600 , 12700 , 12800 , 12900 , 13000 , 13100 , 13200 , 13300 , 13400 , 13500 , 13600 , 13700 , 13800 , 13900 , 14000 , 14100 , 14200 , 14300 , 14400 , 14500 , 14600 , 14700 , 14800 , 14900 , 15000 , 15100 , 15200 , 15300 , 15400 , 15500 , 15600 , 15700 , 15800 , 15900 , 16000 , 16100 , 16200 , 16300 , 16400 , 16500 , 16600 , 16700 , 16800 , 16900 , 17000 , ","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-e54432884de0>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1600\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFEATURES\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/brain-spectrograms/train.pqt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object does not support item assignment"],"ename":"TypeError","evalue":"'DataFrame' object does not support item assignment","output_type":"error"}],"execution_count":28},{"cell_type":"code","source":"# from pyspark.sql import SparkSession\n# from pyspark.sql.types import StructType, StructField, FloatType\n# from pyspark.sql import functions as F\n\n# # Step 1: Create a PySpark DataFrame from `data`\n# num_features = len(FEATURES)\n# feature_schema = StructType([StructField(FEATURES[i], FloatType(), True) for i in range(num_features)])\n\n# # Convert `data` NumPy array to a list of lists\n# data_spark = spark.createDataFrame(data.tolist(), schema=feature_schema)\n\n# # Step 2: Add an index column to train for merging\n# train = train.withColumn(\"index\", F.monotonically_increasing_id())\n# data_spark = data_spark.withColumn(\"index\", F.monotonically_increasing_id())\n\n# # Step 3: Perform an inner join to merge features into train\n# train = train.join(data_spark, on=\"index\", how=\"inner\").drop(\"index\")\n\n# # Step 4: Print final DataFrame shape\n# print(\"New train shape:\", (train.count(), len(train.columns)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:11:24.558071Z","iopub.execute_input":"2025-03-13T18:11:24.55847Z","iopub.status.idle":"2025-03-13T18:11:52.25925Z","shell.execute_reply.started":"2025-03-13T18:11:24.558424Z","shell.execute_reply":"2025-03-13T18:11:52.253697Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-f5bdce1788ae>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Step 4: Print final DataFrame shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"New train shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \"\"\"\n\u001b[0;32m-> 1240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             raise Py4JError(\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 format(target_id, \".\", name))\n","\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o17405.count"],"ename":"Py4JError","evalue":"An error occurred while calling o17405.count","output_type":"error"}],"execution_count":29},{"cell_type":"code","source":" \n# # Paths\n# train_path = \"/kaggle/input/brain-spectrograms/train.pqt\"\n# spec_path = \"/kaggle/input/hms-harmful-brain-activity-classification/train_spectrograms/\"\n\n# # Load data\n# train = spark.read.parquet(train_path)\n# df = spark.read.parquet(spec_path)\n\n# # Verify schema\n# print(\"Train schema:\")\n# train.printSchema()\n# print(\"Spectrogram schema:\")\n# df.printSchema()\n\n# # Ensure `df` has `eeg_id`\n# if \"eeg_id\" not in df.columns:\n#     print(\"Adding eeg_id manually...\")\n#     df = df.withColumn(\"eeg_id\", df[\"spec_id\"])  # If `spec_id` exists\n\n# # Join data correctly\n# df = df.join(train.select(\"eeg_id\"), on=\"eeg_id\", how=\"left\")\n\n# # Feature Engineering\n# spec_cols = [col for col in df.columns if col.startswith(\"f\")]  # Selecting only frequency features\n\n# df_features = df.groupBy(\"eeg_id\").agg(\n#     *(mean(col(f)).alias(f\"{f}_mean_10m\") for f in spec_cols),\n#     *(min(col(f)).alias(f\"{f}_min_10m\") for f in spec_cols),\n#     *(expr(f\"percentile_approx({f}, 0.5)\").alias(f\"{f}_mean_20s\") for f in spec_cols),  # Approx median\n#     *(expr(f\"percentile_approx({f}, 0)\").alias(f\"{f}_min_20s\") for f in spec_cols)     # Approx min\n# )\n\n# # Merge features into train\n# train = train.join(df_features, on=\"eeg_id\", how=\"left\")\n\n# # Show final shape\n# print(f\"New train shape: {train.count()}, {len(train.columns)} features\")\n# train.show(5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:00:13.682986Z","iopub.execute_input":"2025-03-13T17:00:13.68337Z","iopub.status.idle":"2025-03-13T17:00:13.687562Z","shell.execute_reply.started":"2025-03-13T17:00:13.68333Z","shell.execute_reply":"2025-03-13T17:00:13.686416Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# from pyspark.sql import SparkSession\n# from pyspark.sql.functions import mean, min, col\n \n# # Load Data\n# df_train = spark.read.parquet(\"/kaggle/input/brain-spectrograms/train.pqt\")\n# df_spec = spark.read.parquet(f\"{PATH}1000086677.parquet\")\n\n# # Extract spectrogram feature columns (skip the first column, assuming it's an index/time column)\n# spec_cols = df_spec.columns[1:]\n\n \n# # Feature Names\n# features = [f\"{c}_mean_10m\" for c in spec_cols] + [f\"{c}_min_10m\" for c in spec_cols] + \\\n#            [f\"{c}_mean_20s\" for c in spec_cols] + [f\"{c}_min_20s\" for c in spec_cols]\n\n# print(f\"We are creating {len(features)} features for {df_train.count()} rows... \", end=\"\")\n\n \n# # Compute 'r' value for each row in `df_train`\n# df_train = df_train.withColumn(\"r\", expr(\"(min + max) / 4\").cast(\"int\"))\n\n# # Compute Aggregated Features in PySpark\n# df_features = df_spec.alias(\"spec\").join(\n#     df_train.select(\"eeg_id\", \"r\"), on=\"eeg_id\", how=\"inner\"\n# ).groupBy(\"eeg_id\").agg(\n#     *(mean(col(c)).alias(f\"{c}_mean_10m\") for c in spec_cols),\n#     *(min(col(c)).alias(f\"{c}_min_10m\") for c in spec_cols),\n#     *(mean(col(c)).alias(f\"{c}_mean_20s\") for c in spec_cols),\n#     *(min(col(c)).alias(f\"{c}_min_20s\") for c in spec_cols)\n# )\n\n# # Merge engineered features back into `df_train`\n# df_train = df_train.join(df_features, on=\"eeg_id\", how=\"left\")\n\n# # Show output\n# df_train.printSchema()\n# print(\"New train shape:\", (df_train.count(), len(df_train.columns)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:00:58.27557Z","iopub.execute_input":"2025-03-13T17:00:58.275871Z","iopub.status.idle":"2025-03-13T17:00:58.279805Z","shell.execute_reply.started":"2025-03-13T17:00:58.275847Z","shell.execute_reply":"2025-03-13T17:00:58.278731Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#  print(type(train))  # Debugging: Check if train is a DataFrame\n# from pyspark.sql import SparkSession\n# from pyspark.sql.functions import col, mean, min, expr\n\n# # Initialize Spark Session\n# spark = SparkSession.builder.appName(\"EEGFeatureEngineering\").getOrCreate()\n\n# # Load Data\n# df_train = spark.read.parquet(\"/kaggle/input/brain-spectrograms/train.pqt\")\n# df_spec = spark.read.parquet(f\"{PATH}1000086677.parquet\")\n\n \n# # Extract spectrogram feature columns\n# spec_cols = [c for c in df_spec.columns if c != \"eeg_id\"]\n\n# # Ensure 'eeg_id' exists in df_spec\n# if \"eeg_id\" not in df_spec.columns:\n#     print(\"❌ `eeg_id` column is missing in df_spec!\")\n#     # You may need to extract it from another source.\n\n# # Compute 'r' value for each row in `df_train`\n# df_train = df_train.withColumn(\"r\", expr(\"(min + max) / 4\").cast(\"int\"))\n\n# # Perform Aggregation\n# df_features = df_train.alias(\"train\").join(\n#     df_spec.alias(\"spec\"), on=\"eeg_id\" \n# ).groupBy(\"eeg_id\").agg(\n#     *(mean(col(c)).alias(f\"{c}_mean_10m\") for c in spec_cols),\n#     *(min(col(c)).alias(f\"{c}_min_10m\") for c in spec_cols),\n#     *(mean(col(c)).alias(f\"{c}_mean_20s\") for c in spec_cols),\n#     *(min(col(c)).alias(f\"{c}_min_20s\") for c in spec_cols)\n# )\n\n# # Merge features back to df_train\n# df_train = df_train.join(df_features, on=\"eeg_id\", how=\"left\")\n\n# # Show output\n# df_train.printSchema()\n# print(\"✅ New train shape:\", (df_train.count(), len(df_train.columns)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:27:03.928063Z","iopub.execute_input":"2025-03-13T00:27:03.92853Z","iopub.status.idle":"2025-03-13T00:27:04.472957Z","shell.execute_reply.started":"2025-03-13T00:27:03.928491Z","shell.execute_reply":"2025-03-13T00:27:04.471226Z"}},"outputs":[{"name":"stdout","text":"<class 'pyspark.sql.dataframe.DataFrame'>\n❌ `eeg_id` column is missing in df_spec!\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-53cee7485679>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Perform Aggregation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m df_features = df_train.alias(\"train\").join(\n\u001b[0m\u001b[1;32m     26\u001b[0m    \u001b[0mdf_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spec\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eeg_id\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eeg_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how)\u001b[0m\n\u001b[1;32m   2491\u001b[0m                 \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2492\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"how should be a string\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2493\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2494\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_USING_COLUMN_FOR_JOIN] USING column `eeg_id` cannot be resolved on the right side of the join. The right-side columns: [`LL_0`.`59`, `LL_0`.`78`, `LL_0`.`98`, `LL_1`.`17`, `LL_1`.`37`, `LL_1`.`56`, `LL_1`.`76`, `LL_1`.`95`, `LL_10`.`16`, `LL_10`.`35`, `LL_10`.`55`, `LL_10`.`74`, `LL_10`.`94`, `LL_11`.`13`, `LL_11`.`33`, `LL_11`.`52`, `LL_11`.`72`, `LL_11`.`91`, `LL_12`.`11`, `LL_12`.`3`, `LL_12`.`5`, `LL_12`.`7`, `LL_12`.`89`, `LL_13`.`09`, `LL_13`.`28`, `LL_13`.`48`, `LL_13`.`67`, `LL_13`.`87`, `LL_14`.`06`, `LL_14`.`26`, `LL_14`.`45`, `LL_14`.`65`, `LL_14`.`84`, `LL_15`.`04`, `LL_15`.`23`, `LL_15`.`43`, `LL_15`.`63`, `LL_15`.`82`, `LL_16`.`02`, `LL_16`.`21`, `LL_16`.`41`, `LL_16`.`6`, `LL_16`.`8`, `LL_16`.`99`, `LL_17`.`19`, `LL_17`.`38`, `LL_17`.`58`, `LL_17`.`77`, `LL_17`.`97`, `LL_18`.`16`, `LL_18`.`36`, `LL_18`.`55`, `LL_18`.`75`, `LL_18`.`95`, `LL_19`.`14`, `LL_19`.`34`, `LL_19`.`53`, `LL_19`.`73`, `LL_19`.`92`, `LL_2`.`15`, `LL_2`.`34`, `LL_2`.`54`, `LL_2`.`73`, `LL_2`.`93`, `LL_3`.`13`, `LL_3`.`32`, `LL_3`.`52`, `LL_3`.`71`, `LL_3`.`91`, `LL_4`.`1`, `LL_4`.`3`, `LL_4`.`49`, `LL_4`.`69`, `LL_4`.`88`, `LL_5`.`08`, `LL_5`.`27`, `LL_5`.`47`, `LL_5`.`66`, `LL_5`.`86`, `LL_6`.`05`, `LL_6`.`25`, `LL_6`.`45`, `LL_6`.`64`, `LL_6`.`84`, `LL_7`.`03`, `LL_7`.`23`, `LL_7`.`42`, `LL_7`.`62`, `LL_7`.`81`, `LL_8`.`01`, `LL_8`.`2`, `LL_8`.`4`, `LL_8`.`59`, `LL_8`.`79`, `LL_8`.`98`, `LL_9`.`18`, `LL_9`.`38`, `LL_9`.`57`, `LL_9`.`77`, `LL_9`.`96`, `LP_0`.`59`, `LP_0`.`78`, `LP_0`.`98`, `LP_1`.`17`, `LP_1`.`37`, `LP_1`.`56`, `LP_1`.`76`, `LP_1`.`95`, `LP_10`.`16`, `LP_10`.`35`, `LP_10`.`55`, `LP_10`.`74`, `LP_10`.`94`, `LP_11`.`13`, `LP_11`.`33`, `LP_11`.`52`, `LP_11`.`72`, `LP_11`.`91`, `LP_12`.`11`, `LP_12`.`3`, `LP_12`.`5`, `LP_12`.`7`, `LP_12`.`89`, `LP_13`.`09`, `LP_13`.`28`, `LP_13`.`48`, `LP_13`.`67`, `LP_13`.`87`, `LP_14`.`06`, `LP_14`.`26`, `LP_14`.`45`, `LP_14`.`65`, `LP_14`.`84`, `LP_15`.`04`, `LP_15`.`23`, `LP_15`.`43`, `LP_15`.`63`, `LP_15`.`82`, `LP_16`.`02`, `LP_16`.`21`, `LP_16`.`41`, `LP_16`.`6`, `LP_16`.`8`, `LP_16`.`99`, `LP_17`.`19`, `LP_17`.`38`, `LP_17`.`58`, `LP_17`.`77`, `LP_17`.`97`, `LP_18`.`16`, `LP_18`.`36`, `LP_18`.`55`, `LP_18`.`75`, `LP_18`.`95`, `LP_19`.`14`, `LP_19`.`34`, `LP_19`.`53`, `LP_19`.`73`, `LP_19`.`92`, `LP_2`.`15`, `LP_2`.`34`, `LP_2`.`54`, `LP_2`.`73`, `LP_2`.`93`, `LP_3`.`13`, `LP_3`.`32`, `LP_3`.`52`, `LP_3`.`71`, `LP_3`.`91`, `LP_4`.`1`, `LP_4`.`3`, `LP_4`.`49`, `LP_4`.`69`, `LP_4`.`88`, `LP_5`.`08`, `LP_5`.`27`, `LP_5`.`47`, `LP_5`.`66`, `LP_5`.`86`, `LP_6`.`05`, `LP_6`.`25`, `LP_6`.`45`, `LP_6`.`64`, `LP_6`.`84`, `LP_7`.`03`, `LP_7`.`23`, `LP_7`.`42`, `LP_7`.`62`, `LP_7`.`81`, `LP_8`.`01`, `LP_8`.`2`, `LP_8`.`4`, `LP_8`.`59`, `LP_8`.`79`, `LP_8`.`98`, `LP_9`.`18`, `LP_9`.`38`, `LP_9`.`57`, `LP_9`.`77`, `LP_9`.`96`, `RL_0`.`59`, `RL_0`.`78`, `RL_0`.`98`, `RL_1`.`17`, `RL_1`.`37`, `RL_1`.`56`, `RL_1`.`76`, `RL_1`.`95`, `RL_10`.`16`, `RL_10`.`35`, `RL_10`.`55`, `RL_10`.`74`, `RL_10`.`94`, `RL_11`.`13`, `RL_11`.`33`, `RL_11`.`52`, `RL_11`.`72`, `RL_11`.`91`, `RL_12`.`11`, `RL_12`.`3`, `RL_12`.`5`, `RL_12`.`7`, `RL_12`.`89`, `RL_13`.`09`, `RL_13`.`28`, `RL_13`.`48`, `RL_13`.`67`, `RL_13`.`87`, `RL_14`.`06`, `RL_14`.`26`, `RL_14`.`45`, `RL_14`.`65`, `RL_14`.`84`, `RL_15`.`04`, `RL_15`.`23`, `RL_15`.`43`, `RL_15`.`63`, `RL_15`.`82`, `RL_16`.`02`, `RL_16`.`21`, `RL_16`.`41`, `RL_16`.`6`, `RL_16`.`8`, `RL_16`.`99`, `RL_17`.`19`, `RL_17`.`38`, `RL_17`.`58`, `RL_17`.`77`, `RL_17`.`97`, `RL_18`.`16`, `RL_18`.`36`, `RL_18`.`55`, `RL_18`.`75`, `RL_18`.`95`, `RL_19`.`14`, `RL_19`.`34`, `RL_19`.`53`, `RL_19`.`73`, `RL_19`.`92`, `RL_2`.`15`, `RL_2`.`34`, `RL_2`.`54`, `RL_2`.`73`, `RL_2`.`93`, `RL_3`.`13`, `RL_3`.`32`, `RL_3`.`52`, `RL_3`.`71`, `RL_3`.`91`, `RL_4`.`1`, `RL_4`.`3`, `RL_4`.`49`, `RL_4`.`69`, `RL_4`.`88`, `RL_5`.`08`, `RL_5`.`27`, `RL_5`.`47`, `RL_5`.`66`, `RL_5`.`86`, `RL_6`.`05`, `RL_6`.`25`, `RL_6`.`45`, `RL_6`.`64`, `RL_6`.`84`, `RL_7`.`03`, `RL_7`.`23`, `RL_7`.`42`, `RL_7`.`62`, `RL_7`.`81`, `RL_8`.`01`, `RL_8`.`2`, `RL_8`.`4`, `RL_8`.`59`, `RL_8`.`79`, `RL_8`.`98`, `RL_9`.`18`, `RL_9`.`38`, `RL_9`.`57`, `RL_9`.`77`, `RL_9`.`96`, `RP_0`.`59`, `RP_0`.`78`, `RP_0`.`98`, `RP_1`.`17`, `RP_1`.`37`, `RP_1`.`56`, `RP_1`.`76`, `RP_1`.`95`, `RP_10`.`16`, `RP_10`.`35`, `RP_10`.`55`, `RP_10`.`74`, `RP_10`.`94`, `RP_11`.`13`, `RP_11`.`33`, `RP_11`.`52`, `RP_11`.`72`, `RP_11`.`91`, `RP_12`.`11`, `RP_12`.`3`, `RP_12`.`5`, `RP_12`.`7`, `RP_12`.`89`, `RP_13`.`09`, `RP_13`.`28`, `RP_13`.`48`, `RP_13`.`67`, `RP_13`.`87`, `RP_14`.`06`, `RP_14`.`26`, `RP_14`.`45`, `RP_14`.`65`, `RP_14`.`84`, `RP_15`.`04`, `RP_15`.`23`, `RP_15`.`43`, `RP_15`.`63`, `RP_15`.`82`, `RP_16`.`02`, `RP_16`.`21`, `RP_16`.`41`, `RP_16`.`6`, `RP_16`.`8`, `RP_16`.`99`, `RP_17`.`19`, `RP_17`.`38`, `RP_17`.`58`, `RP_17`.`77`, `RP_17`.`97`, `RP_18`.`16`, `RP_18`.`36`, `RP_18`.`55`, `RP_18`.`75`, `RP_18`.`95`, `RP_19`.`14`, `RP_19`.`34`, `RP_19`.`53`, `RP_19`.`73`, `RP_19`.`92`, `RP_2`.`15`, `RP_2`.`34`, `RP_2`.`54`, `RP_2`.`73`, `RP_2`.`93`, `RP_3`.`13`, `RP_3`.`32`, `RP_3`.`52`, `RP_3`.`71`, `RP_3`.`91`, `RP_4`.`1`, `RP_4`.`3`, `RP_4`.`49`, `RP_4`.`69`, `RP_4`.`88`, `RP_5`.`08`, `RP_5`.`27`, `RP_5`.`47`, `RP_5`.`66`, `RP_5`.`86`, `RP_6`.`05`, `RP_6`.`25`, `RP_6`.`45`, `RP_6`.`64`, `RP_6`.`84`, `RP_7`.`03`, `RP_7`.`23`, `RP_7`.`42`, `RP_7`.`62`, `RP_7`.`81`, `RP_8`.`01`, `RP_8`.`2`, `RP_8`.`4`, `RP_8`.`59`, `RP_8`.`79`, `RP_8`.`98`, `RP_9`.`18`, `RP_9`.`38`, `RP_9`.`57`, `RP_9`.`77`, `RP_9`.`96`, `time`]."],"ename":"AnalysisException","evalue":"[UNRESOLVED_USING_COLUMN_FOR_JOIN] USING column `eeg_id` cannot be resolved on the right side of the join. The right-side columns: [`LL_0`.`59`, `LL_0`.`78`, `LL_0`.`98`, `LL_1`.`17`, `LL_1`.`37`, `LL_1`.`56`, `LL_1`.`76`, `LL_1`.`95`, `LL_10`.`16`, `LL_10`.`35`, `LL_10`.`55`, `LL_10`.`74`, `LL_10`.`94`, `LL_11`.`13`, `LL_11`.`33`, `LL_11`.`52`, `LL_11`.`72`, `LL_11`.`91`, `LL_12`.`11`, `LL_12`.`3`, `LL_12`.`5`, `LL_12`.`7`, `LL_12`.`89`, `LL_13`.`09`, `LL_13`.`28`, `LL_13`.`48`, `LL_13`.`67`, `LL_13`.`87`, `LL_14`.`06`, `LL_14`.`26`, `LL_14`.`45`, `LL_14`.`65`, `LL_14`.`84`, `LL_15`.`04`, `LL_15`.`23`, `LL_15`.`43`, `LL_15`.`63`, `LL_15`.`82`, `LL_16`.`02`, `LL_16`.`21`, `LL_16`.`41`, `LL_16`.`6`, `LL_16`.`8`, `LL_16`.`99`, `LL_17`.`19`, `LL_17`.`38`, `LL_17`.`58`, `LL_17`.`77`, `LL_17`.`97`, `LL_18`.`16`, `LL_18`.`36`, `LL_18`.`55`, `LL_18`.`75`, `LL_18`.`95`, `LL_19`.`14`, `LL_19`.`34`, `LL_19`.`53`, `LL_19`.`73`, `LL_19`.`92`, `LL_2`.`15`, `LL_2`.`34`, `LL_2`.`54`, `LL_2`.`73`, `LL_2`.`93`, `LL_3`.`13`, `LL_3`.`32`, `LL_3`.`52`, `LL_3`.`71`, `LL_3`.`91`, `LL_4`.`1`, `LL_4`.`3`, `LL_4`.`49`, `LL_4`.`69`, `LL_4`.`88`, `LL_5`.`08`, `LL_5`.`27`, `LL_5`.`47`, `LL_5`.`66`, `LL_5`.`86`, `LL_6`.`05`, `LL_6`.`25`, `LL_6`.`45`, `LL_6`.`64`, `LL_6`.`84`, `LL_7`.`03`, `LL_7`.`23`, `LL_7`.`42`, `LL_7`.`62`, `LL_7`.`81`, `LL_8`.`01`, `LL_8`.`2`, `LL_8`.`4`, `LL_8`.`59`, `LL_8`.`79`, `LL_8`.`98`, `LL_9`.`18`, `LL_9`.`38`, `LL_9`.`57`, `LL_9`.`77`, `LL_9`.`96`, `LP_0`.`59`, `LP_0`.`78`, `LP_0`.`98`, `LP_1`.`17`, `LP_1`.`37`, `LP_1`.`56`, `LP_1`.`76`, `LP_1`.`95`, `LP_10`.`16`, `LP_10`.`35`, `LP_10`.`55`, `LP_10`.`74`, `LP_10`.`94`, `LP_11`.`13`, `LP_11`.`33`, `LP_11`.`52`, `LP_11`.`72`, `LP_11`.`91`, `LP_12`.`11`, `LP_12`.`3`, `LP_12`.`5`, `LP_12`.`7`, `LP_12`.`89`, `LP_13`.`09`, `LP_13`.`28`, `LP_13`.`48`, `LP_13`.`67`, `LP_13`.`87`, `LP_14`.`06`, `LP_14`.`26`, `LP_14`.`45`, `LP_14`.`65`, `LP_14`.`84`, `LP_15`.`04`, `LP_15`.`23`, `LP_15`.`43`, `LP_15`.`63`, `LP_15`.`82`, `LP_16`.`02`, `LP_16`.`21`, `LP_16`.`41`, `LP_16`.`6`, `LP_16`.`8`, `LP_16`.`99`, `LP_17`.`19`, `LP_17`.`38`, `LP_17`.`58`, `LP_17`.`77`, `LP_17`.`97`, `LP_18`.`16`, `LP_18`.`36`, `LP_18`.`55`, `LP_18`.`75`, `LP_18`.`95`, `LP_19`.`14`, `LP_19`.`34`, `LP_19`.`53`, `LP_19`.`73`, `LP_19`.`92`, `LP_2`.`15`, `LP_2`.`34`, `LP_2`.`54`, `LP_2`.`73`, `LP_2`.`93`, `LP_3`.`13`, `LP_3`.`32`, `LP_3`.`52`, `LP_3`.`71`, `LP_3`.`91`, `LP_4`.`1`, `LP_4`.`3`, `LP_4`.`49`, `LP_4`.`69`, `LP_4`.`88`, `LP_5`.`08`, `LP_5`.`27`, `LP_5`.`47`, `LP_5`.`66`, `LP_5`.`86`, `LP_6`.`05`, `LP_6`.`25`, `LP_6`.`45`, `LP_6`.`64`, `LP_6`.`84`, `LP_7`.`03`, `LP_7`.`23`, `LP_7`.`42`, `LP_7`.`62`, `LP_7`.`81`, `LP_8`.`01`, `LP_8`.`2`, `LP_8`.`4`, `LP_8`.`59`, `LP_8`.`79`, `LP_8`.`98`, `LP_9`.`18`, `LP_9`.`38`, `LP_9`.`57`, `LP_9`.`77`, `LP_9`.`96`, `RL_0`.`59`, `RL_0`.`78`, `RL_0`.`98`, `RL_1`.`17`, `RL_1`.`37`, `RL_1`.`56`, `RL_1`.`76`, `RL_1`.`95`, `RL_10`.`16`, `RL_10`.`35`, `RL_10`.`55`, `RL_10`.`74`, `RL_10`.`94`, `RL_11`.`13`, `RL_11`.`33`, `RL_11`.`52`, `RL_11`.`72`, `RL_11`.`91`, `RL_12`.`11`, `RL_12`.`3`, `RL_12`.`5`, `RL_12`.`7`, `RL_12`.`89`, `RL_13`.`09`, `RL_13`.`28`, `RL_13`.`48`, `RL_13`.`67`, `RL_13`.`87`, `RL_14`.`06`, `RL_14`.`26`, `RL_14`.`45`, `RL_14`.`65`, `RL_14`.`84`, `RL_15`.`04`, `RL_15`.`23`, `RL_15`.`43`, `RL_15`.`63`, `RL_15`.`82`, `RL_16`.`02`, `RL_16`.`21`, `RL_16`.`41`, `RL_16`.`6`, `RL_16`.`8`, `RL_16`.`99`, `RL_17`.`19`, `RL_17`.`38`, `RL_17`.`58`, `RL_17`.`77`, `RL_17`.`97`, `RL_18`.`16`, `RL_18`.`36`, `RL_18`.`55`, `RL_18`.`75`, `RL_18`.`95`, `RL_19`.`14`, `RL_19`.`34`, `RL_19`.`53`, `RL_19`.`73`, `RL_19`.`92`, `RL_2`.`15`, `RL_2`.`34`, `RL_2`.`54`, `RL_2`.`73`, `RL_2`.`93`, `RL_3`.`13`, `RL_3`.`32`, `RL_3`.`52`, `RL_3`.`71`, `RL_3`.`91`, `RL_4`.`1`, `RL_4`.`3`, `RL_4`.`49`, `RL_4`.`69`, `RL_4`.`88`, `RL_5`.`08`, `RL_5`.`27`, `RL_5`.`47`, `RL_5`.`66`, `RL_5`.`86`, `RL_6`.`05`, `RL_6`.`25`, `RL_6`.`45`, `RL_6`.`64`, `RL_6`.`84`, `RL_7`.`03`, `RL_7`.`23`, `RL_7`.`42`, `RL_7`.`62`, `RL_7`.`81`, `RL_8`.`01`, `RL_8`.`2`, `RL_8`.`4`, `RL_8`.`59`, `RL_8`.`79`, `RL_8`.`98`, `RL_9`.`18`, `RL_9`.`38`, `RL_9`.`57`, `RL_9`.`77`, `RL_9`.`96`, `RP_0`.`59`, `RP_0`.`78`, `RP_0`.`98`, `RP_1`.`17`, `RP_1`.`37`, `RP_1`.`56`, `RP_1`.`76`, `RP_1`.`95`, `RP_10`.`16`, `RP_10`.`35`, `RP_10`.`55`, `RP_10`.`74`, `RP_10`.`94`, `RP_11`.`13`, `RP_11`.`33`, `RP_11`.`52`, `RP_11`.`72`, `RP_11`.`91`, `RP_12`.`11`, `RP_12`.`3`, `RP_12`.`5`, `RP_12`.`7`, `RP_12`.`89`, `RP_13`.`09`, `RP_13`.`28`, `RP_13`.`48`, `RP_13`.`67`, `RP_13`.`87`, `RP_14`.`06`, `RP_14`.`26`, `RP_14`.`45`, `RP_14`.`65`, `RP_14`.`84`, `RP_15`.`04`, `RP_15`.`23`, `RP_15`.`43`, `RP_15`.`63`, `RP_15`.`82`, `RP_16`.`02`, `RP_16`.`21`, `RP_16`.`41`, `RP_16`.`6`, `RP_16`.`8`, `RP_16`.`99`, `RP_17`.`19`, `RP_17`.`38`, `RP_17`.`58`, `RP_17`.`77`, `RP_17`.`97`, `RP_18`.`16`, `RP_18`.`36`, `RP_18`.`55`, `RP_18`.`75`, `RP_18`.`95`, `RP_19`.`14`, `RP_19`.`34`, `RP_19`.`53`, `RP_19`.`73`, `RP_19`.`92`, `RP_2`.`15`, `RP_2`.`34`, `RP_2`.`54`, `RP_2`.`73`, `RP_2`.`93`, `RP_3`.`13`, `RP_3`.`32`, `RP_3`.`52`, `RP_3`.`71`, `RP_3`.`91`, `RP_4`.`1`, `RP_4`.`3`, `RP_4`.`49`, `RP_4`.`69`, `RP_4`.`88`, `RP_5`.`08`, `RP_5`.`27`, `RP_5`.`47`, `RP_5`.`66`, `RP_5`.`86`, `RP_6`.`05`, `RP_6`.`25`, `RP_6`.`45`, `RP_6`.`64`, `RP_6`.`84`, `RP_7`.`03`, `RP_7`.`23`, `RP_7`.`42`, `RP_7`.`62`, `RP_7`.`81`, `RP_8`.`01`, `RP_8`.`2`, `RP_8`.`4`, `RP_8`.`59`, `RP_8`.`79`, `RP_8`.`98`, `RP_9`.`18`, `RP_9`.`38`, `RP_9`.`57`, `RP_9`.`77`, `RP_9`.`96`, `time`].","output_type":"error"}],"execution_count":40},{"cell_type":"code","source":"# from pyspark.sql import SparkSession\n# from pyspark.sql.functions import col, mean, min\n# import os\n\n# # # Initialize Spark session\n# # spark = SparkSession.builder.appName(\"EEGFeatureEngineering\").getOrCreate()\n\n# # Define path to spectrogram parquet files\n# PATH = \"/kaggle/input/hms-harmful-brain-activity-classification/train_spectrograms/\"\n\n# # List all parquet files\n# files = [f for f in os.listdir(PATH) if f.endswith(\".parquet\")]\n\n# print(f\"There are {len(files)} spectrogram parquet files.\")\n\n# # Read all parquet files into a single PySpark DataFrame\n# df = spark.read.parquet(PATH)\n\n# # # Select relevant columns (assuming frequency columns are named like 'freq_1', 'freq_2', etc.)\n# # freq_cols = [c for c in df.columns if \"freq_\" in c]\n# freq_cols = [c for c in df.columns if c.startswith((\"LL_\", \"RL_\", \"LP_\", \"RP_\"))]\n\n# # Debug: Print extracted frequency columns\n# # print(\"✅ Frequency Columns:\", freq_cols)\n\n# # If still empty, raise an error\n# if not freq_cols:\n#     raise ValueError(\"❌ No frequency columns found! Check column naming pattern.\")\n\n# # Compute mean and min over 10-min and 20-sec windows\n# df_features = df.groupBy(\"time\").agg(\n#     *(mean(col(f)).alias(f\"{f}_mean\") for f in freq_cols),\n#     *(min(col(f)).alias(f\"{f}_min\") for f in freq_cols)\n# )\n# # \n# # Show result\n# df_features.show(5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:09:55.31869Z","iopub.execute_input":"2025-03-13T16:09:55.319014Z","iopub.status.idle":"2025-03-13T16:10:09.547673Z","shell.execute_reply.started":"2025-03-13T16:09:55.318977Z","shell.execute_reply":"2025-03-13T16:10:09.546353Z"}},"outputs":[{"name":"stdout","text":"There are 11138 spectrogram parquet files.\n✅ Frequency Columns: ['LL_0.59', 'LL_0.78', 'LL_0.98', 'LL_1.17', 'LL_1.37', 'LL_1.56', 'LL_1.76', 'LL_1.95', 'LL_2.15', 'LL_2.34', 'LL_2.54', 'LL_2.73', 'LL_2.93', 'LL_3.13', 'LL_3.32', 'LL_3.52', 'LL_3.71', 'LL_3.91', 'LL_4.1', 'LL_4.3', 'LL_4.49', 'LL_4.69', 'LL_4.88', 'LL_5.08', 'LL_5.27', 'LL_5.47', 'LL_5.66', 'LL_5.86', 'LL_6.05', 'LL_6.25', 'LL_6.45', 'LL_6.64', 'LL_6.84', 'LL_7.03', 'LL_7.23', 'LL_7.42', 'LL_7.62', 'LL_7.81', 'LL_8.01', 'LL_8.2', 'LL_8.4', 'LL_8.59', 'LL_8.79', 'LL_8.98', 'LL_9.18', 'LL_9.38', 'LL_9.57', 'LL_9.77', 'LL_9.96', 'LL_10.16', 'LL_10.35', 'LL_10.55', 'LL_10.74', 'LL_10.94', 'LL_11.13', 'LL_11.33', 'LL_11.52', 'LL_11.72', 'LL_11.91', 'LL_12.11', 'LL_12.3', 'LL_12.5', 'LL_12.7', 'LL_12.89', 'LL_13.09', 'LL_13.28', 'LL_13.48', 'LL_13.67', 'LL_13.87', 'LL_14.06', 'LL_14.26', 'LL_14.45', 'LL_14.65', 'LL_14.84', 'LL_15.04', 'LL_15.23', 'LL_15.43', 'LL_15.63', 'LL_15.82', 'LL_16.02', 'LL_16.21', 'LL_16.41', 'LL_16.6', 'LL_16.8', 'LL_16.99', 'LL_17.19', 'LL_17.38', 'LL_17.58', 'LL_17.77', 'LL_17.97', 'LL_18.16', 'LL_18.36', 'LL_18.55', 'LL_18.75', 'LL_18.95', 'LL_19.14', 'LL_19.34', 'LL_19.53', 'LL_19.73', 'LL_19.92', 'RL_0.59', 'RL_0.78', 'RL_0.98', 'RL_1.17', 'RL_1.37', 'RL_1.56', 'RL_1.76', 'RL_1.95', 'RL_2.15', 'RL_2.34', 'RL_2.54', 'RL_2.73', 'RL_2.93', 'RL_3.13', 'RL_3.32', 'RL_3.52', 'RL_3.71', 'RL_3.91', 'RL_4.1', 'RL_4.3', 'RL_4.49', 'RL_4.69', 'RL_4.88', 'RL_5.08', 'RL_5.27', 'RL_5.47', 'RL_5.66', 'RL_5.86', 'RL_6.05', 'RL_6.25', 'RL_6.45', 'RL_6.64', 'RL_6.84', 'RL_7.03', 'RL_7.23', 'RL_7.42', 'RL_7.62', 'RL_7.81', 'RL_8.01', 'RL_8.2', 'RL_8.4', 'RL_8.59', 'RL_8.79', 'RL_8.98', 'RL_9.18', 'RL_9.38', 'RL_9.57', 'RL_9.77', 'RL_9.96', 'RL_10.16', 'RL_10.35', 'RL_10.55', 'RL_10.74', 'RL_10.94', 'RL_11.13', 'RL_11.33', 'RL_11.52', 'RL_11.72', 'RL_11.91', 'RL_12.11', 'RL_12.3', 'RL_12.5', 'RL_12.7', 'RL_12.89', 'RL_13.09', 'RL_13.28', 'RL_13.48', 'RL_13.67', 'RL_13.87', 'RL_14.06', 'RL_14.26', 'RL_14.45', 'RL_14.65', 'RL_14.84', 'RL_15.04', 'RL_15.23', 'RL_15.43', 'RL_15.63', 'RL_15.82', 'RL_16.02', 'RL_16.21', 'RL_16.41', 'RL_16.6', 'RL_16.8', 'RL_16.99', 'RL_17.19', 'RL_17.38', 'RL_17.58', 'RL_17.77', 'RL_17.97', 'RL_18.16', 'RL_18.36', 'RL_18.55', 'RL_18.75', 'RL_18.95', 'RL_19.14', 'RL_19.34', 'RL_19.53', 'RL_19.73', 'RL_19.92', 'LP_0.59', 'LP_0.78', 'LP_0.98', 'LP_1.17', 'LP_1.37', 'LP_1.56', 'LP_1.76', 'LP_1.95', 'LP_2.15', 'LP_2.34', 'LP_2.54', 'LP_2.73', 'LP_2.93', 'LP_3.13', 'LP_3.32', 'LP_3.52', 'LP_3.71', 'LP_3.91', 'LP_4.1', 'LP_4.3', 'LP_4.49', 'LP_4.69', 'LP_4.88', 'LP_5.08', 'LP_5.27', 'LP_5.47', 'LP_5.66', 'LP_5.86', 'LP_6.05', 'LP_6.25', 'LP_6.45', 'LP_6.64', 'LP_6.84', 'LP_7.03', 'LP_7.23', 'LP_7.42', 'LP_7.62', 'LP_7.81', 'LP_8.01', 'LP_8.2', 'LP_8.4', 'LP_8.59', 'LP_8.79', 'LP_8.98', 'LP_9.18', 'LP_9.38', 'LP_9.57', 'LP_9.77', 'LP_9.96', 'LP_10.16', 'LP_10.35', 'LP_10.55', 'LP_10.74', 'LP_10.94', 'LP_11.13', 'LP_11.33', 'LP_11.52', 'LP_11.72', 'LP_11.91', 'LP_12.11', 'LP_12.3', 'LP_12.5', 'LP_12.7', 'LP_12.89', 'LP_13.09', 'LP_13.28', 'LP_13.48', 'LP_13.67', 'LP_13.87', 'LP_14.06', 'LP_14.26', 'LP_14.45', 'LP_14.65', 'LP_14.84', 'LP_15.04', 'LP_15.23', 'LP_15.43', 'LP_15.63', 'LP_15.82', 'LP_16.02', 'LP_16.21', 'LP_16.41', 'LP_16.6', 'LP_16.8', 'LP_16.99', 'LP_17.19', 'LP_17.38', 'LP_17.58', 'LP_17.77', 'LP_17.97', 'LP_18.16', 'LP_18.36', 'LP_18.55', 'LP_18.75', 'LP_18.95', 'LP_19.14', 'LP_19.34', 'LP_19.53', 'LP_19.73', 'LP_19.92', 'RP_0.59', 'RP_0.78', 'RP_0.98', 'RP_1.17', 'RP_1.37', 'RP_1.56', 'RP_1.76', 'RP_1.95', 'RP_2.15', 'RP_2.34', 'RP_2.54', 'RP_2.73', 'RP_2.93', 'RP_3.13', 'RP_3.32', 'RP_3.52', 'RP_3.71', 'RP_3.91', 'RP_4.1', 'RP_4.3', 'RP_4.49', 'RP_4.69', 'RP_4.88', 'RP_5.08', 'RP_5.27', 'RP_5.47', 'RP_5.66', 'RP_5.86', 'RP_6.05', 'RP_6.25', 'RP_6.45', 'RP_6.64', 'RP_6.84', 'RP_7.03', 'RP_7.23', 'RP_7.42', 'RP_7.62', 'RP_7.81', 'RP_8.01', 'RP_8.2', 'RP_8.4', 'RP_8.59', 'RP_8.79', 'RP_8.98', 'RP_9.18', 'RP_9.38', 'RP_9.57', 'RP_9.77', 'RP_9.96', 'RP_10.16', 'RP_10.35', 'RP_10.55', 'RP_10.74', 'RP_10.94', 'RP_11.13', 'RP_11.33', 'RP_11.52', 'RP_11.72', 'RP_11.91', 'RP_12.11', 'RP_12.3', 'RP_12.5', 'RP_12.7', 'RP_12.89', 'RP_13.09', 'RP_13.28', 'RP_13.48', 'RP_13.67', 'RP_13.87', 'RP_14.06', 'RP_14.26', 'RP_14.45', 'RP_14.65', 'RP_14.84', 'RP_15.04', 'RP_15.23', 'RP_15.43', 'RP_15.63', 'RP_15.82', 'RP_16.02', 'RP_16.21', 'RP_16.41', 'RP_16.6', 'RP_16.8', 'RP_16.99', 'RP_17.19', 'RP_17.38', 'RP_17.58', 'RP_17.77', 'RP_17.97', 'RP_18.16', 'RP_18.36', 'RP_18.55', 'RP_18.75', 'RP_18.95', 'RP_19.14', 'RP_19.34', 'RP_19.53', 'RP_19.73', 'RP_19.92']\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-f12b7b3cde56>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Compute mean and min over 10-min and 20-sec windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m df_features = df.groupBy(\"time\").agg(\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{f}_mean\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfreq_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{f}_min\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfreq_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/group.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"all exprs should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mexprs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `LL_0`.`59` cannot be resolved. Did you mean one of the following? [`LL_0.59`, `LL_8.59`, `LP_0.59`, `RL_0.59`, `LL_0.78`].;\n'Aggregate [time#8563L], [time#8563L, avg('LL_0.59) AS LL_0.59_mean#9767, avg('LL_0.78) AS LL_0.78_mean#9769, avg('LL_0.98) AS LL_0.98_mean#9771, avg('LL_1.17) AS LL_1.17_mean#9773, avg('LL_1.37) AS LL_1.37_mean#9775, avg('LL_1.56) AS LL_1.56_mean#9777, avg('LL_1.76) AS LL_1.76_mean#9779, avg('LL_1.95) AS LL_1.95_mean#9781, avg('LL_2.15) AS LL_2.15_mean#9783, avg('LL_2.34) AS LL_2.34_mean#9785, avg('LL_2.54) AS LL_2.54_mean#9787, avg('LL_2.73) AS LL_2.73_mean#9789, avg('LL_2.93) AS LL_2.93_mean#9791, avg('LL_3.13) AS LL_3.13_mean#9793, avg('LL_3.32) AS LL_3.32_mean#9795, avg('LL_3.52) AS LL_3.52_mean#9797, avg('LL_3.71) AS LL_3.71_mean#9799, avg('LL_3.91) AS LL_3.91_mean#9801, avg('LL_4.1) AS LL_4.1_mean#9803, avg('LL_4.3) AS LL_4.3_mean#9805, avg('LL_4.49) AS LL_4.49_mean#9807, avg('LL_4.69) AS LL_4.69_mean#9809, avg('LL_4.88) AS LL_4.88_mean#9811, ... 777 more fields]\n+- Relation [time#8563L,LL_0.59#8564,LL_0.78#8565,LL_0.98#8566,LL_1.17#8567,LL_1.37#8568,LL_1.56#8569,LL_1.76#8570,LL_1.95#8571,LL_2.15#8572,LL_2.34#8573,LL_2.54#8574,LL_2.73#8575,LL_2.93#8576,LL_3.13#8577,LL_3.32#8578,LL_3.52#8579,LL_3.71#8580,LL_3.91#8581,LL_4.1#8582,LL_4.3#8583,LL_4.49#8584,LL_4.69#8585,LL_4.88#8586,... 377 more fields] parquet\n"],"ename":"AnalysisException","evalue":"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `LL_0`.`59` cannot be resolved. Did you mean one of the following? [`LL_0.59`, `LL_8.59`, `LP_0.59`, `RL_0.59`, `LL_0.78`].;\n'Aggregate [time#8563L], [time#8563L, avg('LL_0.59) AS LL_0.59_mean#9767, avg('LL_0.78) AS LL_0.78_mean#9769, avg('LL_0.98) AS LL_0.98_mean#9771, avg('LL_1.17) AS LL_1.17_mean#9773, avg('LL_1.37) AS LL_1.37_mean#9775, avg('LL_1.56) AS LL_1.56_mean#9777, avg('LL_1.76) AS LL_1.76_mean#9779, avg('LL_1.95) AS LL_1.95_mean#9781, avg('LL_2.15) AS LL_2.15_mean#9783, avg('LL_2.34) AS LL_2.34_mean#9785, avg('LL_2.54) AS LL_2.54_mean#9787, avg('LL_2.73) AS LL_2.73_mean#9789, avg('LL_2.93) AS LL_2.93_mean#9791, avg('LL_3.13) AS LL_3.13_mean#9793, avg('LL_3.32) AS LL_3.32_mean#9795, avg('LL_3.52) AS LL_3.52_mean#9797, avg('LL_3.71) AS LL_3.71_mean#9799, avg('LL_3.91) AS LL_3.91_mean#9801, avg('LL_4.1) AS LL_4.1_mean#9803, avg('LL_4.3) AS LL_4.3_mean#9805, avg('LL_4.49) AS LL_4.49_mean#9807, avg('LL_4.69) AS LL_4.69_mean#9809, avg('LL_4.88) AS LL_4.88_mean#9811, ... 777 more fields]\n+- Relation [time#8563L,LL_0.59#8564,LL_0.78#8565,LL_0.98#8566,LL_1.17#8567,LL_1.37#8568,LL_1.56#8569,LL_1.76#8570,LL_1.95#8571,LL_2.15#8572,LL_2.34#8573,LL_2.54#8574,LL_2.73#8575,LL_2.93#8576,LL_3.13#8577,LL_3.32#8578,LL_3.52#8579,LL_3.71#8580,LL_3.91#8581,LL_4.1#8582,LL_4.3#8583,LL_4.49#8584,LL_4.69#8585,LL_4.88#8586,... 377 more fields] parquet\n","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"# print(\"Frequency Columns:\", freq_cols)\n# print(\"Available Columns:\", df.columns)\n# # Identify spectrogram feature columns (exclude non-numeric and timestamp)\n# freq_cols = [c for c in df.columns if c.startswith((\"LL_\", \"RL_\", \"LP_\", \"RP_\"))]\n\n# # Debug: Print extracted frequency columns\n# print(\"✅ Frequency Columns:\", freq_cols)\n\n# # If still empty, raise an error\n# if not freq_cols:\n#     raise ValueError(\"❌ No frequency columns found! Check column naming pattern.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:57:51.755754Z","iopub.execute_input":"2025-03-13T16:57:51.756086Z","iopub.status.idle":"2025-03-13T16:57:51.759845Z","shell.execute_reply.started":"2025-03-13T16:57:51.756061Z","shell.execute_reply":"2025-03-13T16:57:51.758934Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}